# CXPlain: Causal Explanations for Model Interpretation under Uncertainty

*Patrick Schwab, Walter Karlen*  
*2019*

## Introduction
What this paper is about

- A feature importance estimation method

Why feature importance estimation is difficult

- There exists a wide variety of intricate machine learning models with different underlying model structures, algorithms, and decision functions, which change how it should be computed.
- Feature importance estimation is typically associated with significant uncertainty.

What is awesome about the method

- Significantly more accurate than exsiting methods. Accuracy of feature importance? Yes, because it's an *estimation*.
- Can be used for any type of machine learning model.
- Provides uncertainty estimates via bootstrap resampling.
- Fast at *evaluation time*.

## Related Work
Existing feature importance estimation methods can be subdivided into four categories:

1. Gradient-based methods
2. Methods based on sensitivity analysis
3. Methods that measure the change in model confidence when removing input feature
4. Mimic models

Some examples for each category:

1. Simple Gradient, Integrated Gradients, DeepLIFT, DeepSHAP
2. LIME, SHAP
3. Conditional multivariate models, Image masking models
4. Tree-based models, rule-based models

Limitation by category:

1. Only applicable to differentiable models.
2. Slow.
3. (Not mentioned, maybe because this is the category CXPlain falls into.)
4. Not guaranteed to match the behavior of the original model.

On top of these limitations in each category, most of them don't inform users when their estimates are significantly uncertain and can not be expected to be accurate.

## Main Idea
Train a separate explanation model $\hat{f}\_{exp}$ to explain the predictive model $\hat{f}$ which you want to estimate feature importance on. To train the explanation model $\hat{f}\_{exp}$, a causal objective function is utilized to quantify the marginal contribution of a single input feature $X_{i}$ (or a group of input features) towards the predictive modelâ€™s accuracy.  
The causal objective function looks at an increase of error $\mathcal{L}\(y, \hat{y}\)$ when a specific variable $X_{i}$ is masked and defines it as the cause that the variable $X_{i}$ contributes to the prediction of $\hat{f} \to \hat{y}$. In other words, the prediction target of $\hat{f}\_{exp}$ is the increase of error $\Delta\mathcal{L}$. What's neat about this causal framework is that if you normalize $\Delta\mathcal{L}$ by dividing its sum of all $k$ variable losses, $X$ contains all relevant variables for the causal problem, which is a key assumption of Granger's definition of causality.  
To also quantiy uncertainty of the estimate (prediction of $\hat{f}\_{exp}$), repeat the training of $\hat{f}\_{exp}$ a certain number of times by bootstrap sampling the input from $X$ every time. Then the distribution of the sampling median of the prediction can be used to compute the confidence interval of feature importance.

## Limitations
It's fast at evaluation time, but takes some time to train $\hat{f}\_{exp}$ a cerain number of times to quantify uncertainty.
